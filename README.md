# RealTimeâ€‘VLM

**RealTime-VLM** brings real-time VLM inference to the browser. It continuously captures webcam frames, sends image+text to an OpenAI-compatible API, and displays responses with sub-second latency. Works with local or hosted VLMs; auto-discovers models from */v1/models* and uses */v1/chat/completions* for inference.

---

## âœ¨ Highlights

<p align="center">
  <img src="img/RealTime-VLM_interface.png" alt="RealTime-VLM Interface" width="100%">
</p>


- **Dropâ€‘in UI**: modern glass style, dark mode, keyboard toggle (Space), request log, copy/clear, status badges, FPS.
- **Camera controls**: pick device, resolution (480p/720p/1080p/auto), and JPEG quality.
- **API controls**: base URL, **model selector** (autoâ€‘fetch from `/v1/models`), custom model ID, interval, max tokens, temperature, overlay/autoscroll, â€œTest APIâ€ button.
- **Oneâ€‘shot or live**: `Send once` or stream frames on an interval.
- **Zero build**: static files (`index.html`, `styles.css`, `app.js`) â€” host anywhere that serves HTTPS or use `localhost`.
- **Standardsâ€‘friendly**: uses `getUserMedia`, `canvas.toDataURL`, and OpenAIâ€‘compatible **Chat Completions** with image content.

> Inspired by **smolvlmâ€‘realtimeâ€‘webcam** (llama.cpp + SmolVLM demo), but extended with a richer UX, model picker, and advanced options.

---

## ğŸ§© Architecture
<p align="center">
  <picture>
    <img src="img/RealTime-VLM-architecture.svg" alt="RealTime-VLM Architecture" width="100%">
  </picture>
</p>


---
## ğŸš€ Quick start (1â€‘minute, llama.cpp)

1) **Start a realâ€‘time VLM server** (example: SmolVLM via `llama.cpp`):
```bash
# Fast local VLM (GPU optional with -ngl 99)
llama-server -hf ggml-org/SmolVLM-500M-Instruct-GGUF  # default port: 8080
```

2) **Open the UI**  
- Easiest: just open `index.html` in your browser.  
- If your browser blocks the camera on `file://`, serve the folder locally:
```bash
python3 -m http.server 8081    # then open http://localhost:8081
```

3) **Use the app**  
- Base API defaults to `http://localhost:8080` (change if needed).  
- Click **Test API**, pick a **Model** from `/v1/models` (or choose **Customâ€¦**).  
- Hit **Start** (or **Send once**) and watch responses stream back in real time.

---

### Other backends (optional)

- **vLLM (OpenAIâ€‘compatible)**
```bash
vllm serve mistralai/Pixtral-12B-2409 --host 0.0.0.0 --port 8000
# Base API: http://localhost:8000
```
- **Ollama (OpenAIâ€‘compatible)**
```bash
# Example models vary by support: llama3.2-vision, pixtral, llava, qwen2-vl, etc.
ollama run llama3.2-vision
# Base API: http://localhost:11434/v1
```

## âœ… API contract (what the app expects)

- **List models**: `GET /v1/models` â†’ returns `{ data: [{ id: "..." }, ...] }` (or `["id", ...]` is also accepted).
- **Chat completions**: `POST /v1/chat/completions` with **image_url** input, e.g.:
```jsonc
{
  "model": "your-vision-model-id",
  "max_tokens": 100,
  "temperature": 0.2,
  "messages": [
    { "role": "user", "content": [
      { "type": "text", "text": "What do you see?" },
      { "type": "image_url", "image_url": { "url": "data:image/jpeg;base64,..." } }
    ]}
  ]
}
```
- The app reads the text at `choices[0].message.content` in the response.

If your server uses a slightly different schema, adapt `app.js` in `sendChatCompletionRequest(...)`.

---

## ğŸ§  Supported models (examples)

RealTimeâ€‘VLM works with any **vision** model reachable behind an **OpenAIâ€‘compatible** chat endpoint. Popular choices:

### Local / Openâ€‘weights
- **SmolVLMâ€‘500M Instruct (GGUF)** via llama.cpp â€” tiny & fast for demos.
- **LLaVA / LLaVAâ€‘NeXT** (Llamaâ€‘3 / Qwenâ€‘1.5 backbones) â€” strong general VLM baselines.
- **Llama 3.2â€‘Vision (11B / 90B)** â€” robust open vision models from Meta.
- **Phiâ€‘3.5â€‘Visionâ€‘Instruct** â€” lightweight multimodal model from Microsoft.
- **Pixtralâ€‘12B** â€” Mistralâ€™s multimodal 12B with strong doc understanding.
- **Qwenâ€‘VL / Qwen2â€‘VL / Qwen2.5â€‘VL (3B/7B/32B/72B)** â€” highâ€‘quality VLM family with videoâ€‘length support in newer versions.
- **Molmo (1B/7B/72B)** â€” Ai2â€™s open multimodal models, competitive at their sizes.

> Availability and exact IDs vary by distribution (HF, ModelScope, Ollama, vLLM, llama.cpp). Use the **Model** dropdown (fetches `/v1/models`) or enter a **Custom** ID.

### Hosted APIs (examples)
- **OpenRouter**, **vLLM**/**TGI** gateways, or vendor endpoints that expose **OpenAIâ€‘style** `/v1` endpoints with image support.

---

## ğŸ› ï¸ Configuration tips

- **Interval**: start at **500 ms**. If your model is fast (GPU/quantized), try **250 ms**.  
- **Resolution**: higher = clearer content, but more bandwidth & latency.  
- **JPEG quality**: 0.75â€“0.85 is a good tradeâ€‘off.  
- **Max tokens**: limit to reduce latency.  
- **Temperature**: keep **0.0â€“0.4** for crisp, factual outputs.  
- **Instruction**: steer the model (e.g., *â€œReturn JSON with detected objects and bounding boxes.â€*).

---

## ğŸ”’ Privacy & security

- Camera frames are encoded **in the browser** and sent only to the API you configure.  
- Use **HTTPS** (or `localhost`) so `getUserMedia` works.  
- If selfâ€‘hosting the API, enable **CORS** for your origin and prefer **HTTP/2 + TLS**.  
- Avoid sending sensitive live video to untrusted endpoints.

---

## ğŸ§ª Troubleshooting

- **Camera blocked**: Must be on **HTTPS** or `localhost`. Check browser permissions.  
- **CORS failures**: Configure your server to allow your origin and the `Content-Type` header.  
- **No models in dropdown**: Your server must implement `GET /v1/models`.  
- **No response**: Ensure your model **supports images** and the **Chat Completions** schema.  
- **Latency too high**: Lower resolution/quality, increase interval, or use GPU/quantized weights.

---

## ğŸ“¦ Project structure

```
.
â”œâ”€ index.html    # UI skeleton & layout
â”œâ”€ styles.css    # Aesthetic (glass) styling + dark theme
â”œâ”€ app.js        # Camera, API calls, model fetch, UX logic
â””â”€ README.md
```

---

## ğŸ—ºï¸ Roadmap

- [ ] Streaming responses (SSE / `stream: true`)  
- [ ] Multiâ€‘frame batching  
- [ ] Boundingâ€‘box overlays for detectorâ€‘style prompts  
- [ ] JSON schema validation helpers  
- [ ] Minimal Node proxy with secure CORS presets

---

## ğŸ“š References & model hubs (a few starting points)

- **smolvlmâ€‘realtimeâ€‘webcam** (original demo) â€” llama.cpp + SmolVLM 500M  
  - https://github.com/ngxson/smolvlm-realtime-webcam
- **OpenAI model docs** (GPTâ€‘4o & model catalog)  
  - https://platform.openai.com/docs/models
- **vLLM** â€” OpenAIâ€‘compatible server mode  
  - https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html
- **Ollama** â€” OpenAIâ€‘compatible endpoint  
  - https://ollama.com/blog/openai-compatibility
- **LLaVA / LLaVAâ€‘NeXT**  
  - https://github.com/haotian-liu/LLaVA
- **Llama 3.2 Vision** (Meta)  
  - https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/
- **Phiâ€‘3.5â€‘Vision** (Microsoft)  
  - https://huggingface.co/microsoft/Phi-3.5-vision-instruct
- **Pixtralâ€‘12B** (Mistral)  
  - https://mistral.ai/news/pixtral-12b
- **Qwen2â€‘VL / Qwen2.5â€‘VL**  
  - https://qwenlm.github.io/blog/qwen2-vl/  
  - https://qwenlm.github.io/blog/qwen2.5-vl/
- **Molmo** (Ai2)  
  - https://allenai.org/blog/molmo

---

## ğŸ“ License

MIT â€” do whatever you want, just donâ€™t remove attribution and be kind.

---

## â¤ï¸ Acknowledgements

Thanks to the authors and communities behind **llama.cpp**, **vLLM**, **Ollama**, **LLaVA**, **Meta Llama**, **Microsoft Phi**, **Qwen**, **Mistral**, and **Ai2 Molmo** for pushing open and accessible multimodal research forward.
